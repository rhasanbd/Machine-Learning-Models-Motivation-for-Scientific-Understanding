# Machine Learning Classifier Models: Motivation for Scientific Understanding


In this repository, we address the following question:

- Given a problem, how do we choose the best machine learning (ML) model and its hyperparameters (these are model parameters that are required to be set for learning patterns from training data)?

More specifically, we emphasize the need to acquire a scientific understanding of the ML models. A lack of this knowledge limits our ability to use ML for solving practical problems in two ways:

- We don't know how to select the best model
- We don't know how to find optimal hyperparameters of a chosen model

Due to this knowledge gap, we resort to the brute-force technique of trying every possible ML model and searching over every possible combination of the hyperparameter values. This is not science, and nor is it feasible for large and complex problems.

The goal of this notebook series is to convince ourselves about **the need to acquire a scientific understanding of the ML models**, which will lead to the knowledge of the science of optimal training of the models. Also, we should be able to realize that in practical scenarios problems are significantly more challenging, and thus require a deeper understanding of the models.

In the two notebooks, we will explore 10 ML classifier models:
1. k-Nearest Neighbors (k-NN)
2. Logistic Regression
3. Polynomial Logistic Regression
4. Naive Bayes
5. Linear Support Vector Machine (Linear SVM)
6. Non-Linear Support Vector Machine (SVM Gaussian Radial Basis Function)
7. Decision Tree
8. Ensemble Method: Random Forest
9. Ensemble Method: Voting Classifier
10. Multi-Layer Perceptron (MLP)

