# Machine Learning Classifier Models: Motivation for Scientific Understanding


In this repository we address the following question:

- Given a problem (dataset), how do we choose the best ML model and its hyperparameters (these are model parameters that are required to be set for learning patterns from training data)?

More specifically, we emphasize the need for acquiring a scientific understanding of the ML models. Lack of this knowledge limits our ability to use ML for solving practical problems in two ways:

- We don't know how to select the best model
- We don't know how to find optimal hyperparameters of a chosen model

Due to this knowledge gap we resort to the brute-force technique of trying every possible ML model and search over every possible combination of the hyperparameter values. This is not science, and nor is it feasible for large and complex problems.

The goal of this notebook series is to convince ourselves about **the need for acquiring a scientific understanding of the ML models**, which will lead towards the knowledge of the science of optimal training of the models.

In the two notebooks we will explore 10 Machine Learning (ML) classifier models:
1. K-Nearest Neighbors (KNN)
2. Logistic Regression
3. Polynomial Logistic Regression
4. Naive Bayes
5. Linear Support Vector Machine (Linear SVM)
6. Non-Linear Support Vector Machine (SVM Gaussian Radial Basis Function)
7. Decision Tree
8. Ensemble Method: Random Forest
9. Ensemble Method: Voting Clasifier
10. Multi-Layer Perceptron (MLP)



After reviewing the two notebooks we should be able to convince ourselves **the need for acquiring a scientific understanding of these ML models**. Also we should be able to realize that in practical scenarions problems are significantly more challenging, thus requires deeper understanding of the models.
