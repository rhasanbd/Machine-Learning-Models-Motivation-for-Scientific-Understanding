{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classifier Models: A Brief Tour II\n",
    "\n",
    "In the previous notebook we experimented with following 10 Machine Learning (ML) classifier models on a synthetic two-dimensional dataset. \n",
    "\n",
    "1. K-Nearest Neighbors (KNN)\n",
    "2. Logistic Regression\n",
    "3. Polynomial Logistic Regression\n",
    "4. Naive Bayes\n",
    "5. Linear Support Vector Machine (Linear SVM)\n",
    "6. Non-Linear Support Vector Machine (SVM Gaussian Radial Basis Function)\n",
    "7. Decision Tree\n",
    "8. Ensemble Method: Random Forest\n",
    "9. Ensemble Method: Voting Clasifier\n",
    "10. Multi-Layer Perceptron (MLP)\n",
    "\n",
    "\n",
    "\n",
    "Although the dataset was non-linear, we were able to achieve approximately 95% test accuracy. However, in pratical problems the datasets are often high-dimensional and significantly more non-linear. As a consequence achieving over 90% test accuracy is challenging. \n",
    "\n",
    "In this notebook we will understand this challenge by training the above models with a **high-dimensional large dataset**. We will see that even with the best performing model frok the previous notebook we could not achieve 85% test accuracy.\n",
    "\n",
    "The takeaway lesson from the previous notebook was to appreciate the need for acquiring a scientific understanding of the models and their hyperparameter setting. The current notebook reinforces this lesson by underscoring the fact that **in practical scenarions problems are significantly more challenging**, thus requires a scientic understanding of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Dimensional Dataset\n",
    "\n",
    "URL: https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "\n",
    "The dataset is related to the white variants of the Portuguese \"Vinho Verde\" wine. It provides the physicochemical (inputs) and sensory (the output) variables are available.\n",
    "\n",
    "The dataset can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g., there are much more normal wines than excellent or poor ones).\n",
    "\n",
    "The dataset is **11 dimensional**, which consists of characteristics of white wine (e.g., alcohol content, density, amount of citric acid, pH, etc) with target variable \"quality\" representing rating of wine.\n",
    "\n",
    "The target variable (\"quality\" rating of wine) ranges from 3 to 8. We will convert it into a two-category variable consisting of \"good\" (quality > 5) & \"bad\" (quality <= 5). The target vector should have 0s (representing “bad” quality wine) and 1s (representing “good” quality wine).\n",
    "\n",
    "Given the characteristics of a new, unlabeled wine, the classification task is to predict its \"quality\" (0 or 1).\n",
    "\n",
    "Input variables (based on physicochemical tests):\n",
    "- fixed acidity\n",
    "- volatile acidity\n",
    "- citric acid\n",
    "- residual sugar\n",
    "- chlorides\n",
    "- free sulfur dioxide\n",
    "- total sulfur dioxide\n",
    "- density\n",
    "- pH\n",
    "- sulphates\n",
    "- alcohol\n",
    "\n",
    "Output variable (based on sensory data): \n",
    "- quality (score between 0 and 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file containing the dataset as a Pandas DataFrame object\n",
    "df = pd.read_csv('/Users/hasan/datasets/winequality-white.csv')\n",
    "\n",
    "# Get the target column from the DataFrame\n",
    "y_quality = df['quality'] # 1D targer vector\n",
    "\n",
    "# Create the Target Vector\n",
    "y = (y_quality > 5).astype(np.int)  # 1 if Good Wine, else 0 (Bad Wine)\n",
    "\n",
    "# Create the data matrix containing all features excluding the target\n",
    "X = df.drop(columns='quality')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3918, 11)\n",
      "(980, 11)\n",
      "(3918,)\n",
      "(980,)\n"
     ]
    }
   ],
   "source": [
    "# Spilt the dataset into training and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.7570188871873405\n",
      "Test Accuracy:  0.686734693877551\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "    \n",
    "# Fit the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the training set\n",
    "train_accuracy_knn = knn.score(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the test set\n",
    "test_accuracy_knn = knn.score(X_test, y_test) \n",
    "\n",
    "print(\"Train Accuracy: \", train_accuracy_knn)\n",
    "print(\"Test Accuracy: \", test_accuracy_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.7570188871873405\n",
      "Test Accuracy:  0.726530612244898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "lg_reg = LogisticRegression()\n",
    "    \n",
    "# Fit the model\n",
    "lg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the training set\n",
    "train_accuracy_lg_reg = lg_reg.score(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the test set\n",
    "test_accuracy_lg_reg = lg_reg.score(X_test, y_test) \n",
    "\n",
    "print(\"Train Accuracy: \", train_accuracy_lg_reg)\n",
    "print(\"Test Accuracy: \", test_accuracy_lg_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.7856049004594181\n",
      "Test Accuracy:  0.7622448979591837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "optimal_poly_degree = 3\n",
    "\n",
    "\n",
    "# Create the model\n",
    "lg_reg_poly = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=optimal_poly_degree)), # Add polynomial terms with the feature vector\n",
    "    (\"scaler\", StandardScaler()), # Scale the features\n",
    "    (\"clf\", LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    \n",
    "# Fit the model\n",
    "lg_reg_poly.fit(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the training set\n",
    "train_accuracy_lg_reg_poly = lg_reg_poly.score(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the test set\n",
    "test_accuracy_lg_reg_poly = lg_reg_poly.score(X_test, y_test) \n",
    "\n",
    "print(\"Train Accuracy: \", train_accuracy_lg_reg_poly)\n",
    "print(\"Test Accuracy: \", test_accuracy_lg_reg_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.7120980091883614\n",
      "Test Accuracy:  0.6826530612244898\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Fit the model\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the training set\n",
    "train_accuracy_nb = nb.score(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the test set\n",
    "test_accuracy_nb = nb.score(X_test, y_test) \n",
    "\n",
    "print(\"Train Accuracy: \", train_accuracy_nb)\n",
    "print(\"Test Accuracy: \", test_accuracy_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.7549770290964778\n",
      "Test Accuracy:  0.7306122448979592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create the model: Linear SVM\n",
    "svm_linear = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_svc\", LinearSVC()),\n",
    "    ])\n",
    "\n",
    "# Fit the model\n",
    "svm_linear.fit(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the training set\n",
    "train_accuracy_svm_linear = svm_linear.score(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the test set\n",
    "test_accuracy_svm_linear = svm_linear.score(X_test, y_test) \n",
    "\n",
    "print(\"Train Accuracy: \", train_accuracy_svm_linear)\n",
    "print(\"Test Accuracy: \", test_accuracy_svm_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linear Support Vector Machine (Gaussian Radial Basis Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  1.0\n",
      "Test Accuracy:  0.7326530612244898\n"
     ]
    }
   ],
   "source": [
    "# Create the model: Gaussian Radial Basis Function (RBF) based SVM\n",
    "svm = SVC(kernel=\"rbf\", gamma=0.3, C=100)\n",
    "\n",
    "# Fit the model\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the training set\n",
    "train_accuracy_svm = svm.score(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the test set\n",
    "test_accuracy_svm = svm.score(X_test, y_test) \n",
    "\n",
    "print(\"Train Accuracy: \", train_accuracy_svm)\n",
    "print(\"Test Accuracy: \", test_accuracy_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9997447677386422\n",
      "Test Accuracy:  0.7714285714285715\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "dtree = DecisionTreeClassifier(max_depth=20)\n",
    "    \n",
    "# Fit the model\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the training set\n",
    "train_accuracy_dtree = dtree.score(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the test set\n",
    "test_accuracy_dtree = dtree.score(X_test, y_test) \n",
    "\n",
    "print(\"Train Accuracy: \", train_accuracy_dtree)\n",
    "print(\"Test Accuracy: \", test_accuracy_dtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Mathod: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  1.0\n",
      "Test Accuracy:  0.8255102040816327\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "rndforest = RandomForestClassifier(n_estimators=100, max_depth=20)\n",
    "    \n",
    "# Fit the model\n",
    "rndforest.fit(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the training set\n",
    "train_accuracy_rndforest = rndforest.score(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the test set\n",
    "test_accuracy_rndforest = rndforest.score(X_test, y_test) \n",
    "\n",
    "print(\"Train Accuracy: \", train_accuracy_rndforest)\n",
    "print(\"Test Accuracy: \", test_accuracy_rndforest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Method: Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.9561000510464522\n",
      "Test Accuracy:  0.786734693877551\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('K-NN', knn), ('Naive Bayes', nb), \n",
    "                ('Support Vector Machine', svm), ('Random Forest', rndforest)],\n",
    "    voting='hard')\n",
    "\n",
    "# Fit the model\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the training set\n",
    "train_accuracy_voting_clf = voting_clf.score(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the test set\n",
    "test_accuracy_voting_clf = voting_clf.score(X_test, y_test) \n",
    "\n",
    "print(\"Train Accuracy: \", train_accuracy_voting_clf)\n",
    "print(\"Test Accuracy: \", test_accuracy_voting_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network (Multi-Layer Perceptron)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  1.0\n",
      "Test Accuracy:  0.7969387755102041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hasan/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "mlp = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"MLP\", MLPClassifier(hidden_layer_sizes=(300,100), alpha=0.2, \n",
    "                              activation='relu', solver='lbfgs',\n",
    "                              early_stopping=True, n_iter_no_change=10, max_iter=1000)),\n",
    "    ])\n",
    "    \n",
    "# Fit the model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the training set\n",
    "train_accuracy_mlp = mlp.score(X_train, y_train)\n",
    "\n",
    "# Compute accuracy on the test set\n",
    "test_accuracy_mlp = mlp.score(X_test, y_test) \n",
    "\n",
    "print(\"Train Accuracy: \", train_accuracy_mlp)\n",
    "print(\"Test Accuracy: \", test_accuracy_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison of 9 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.757019</td>\n",
       "      <td>0.686735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.757019</td>\n",
       "      <td>0.726531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polynomial Logistic Regression</td>\n",
       "      <td>0.785605</td>\n",
       "      <td>0.762245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.712098</td>\n",
       "      <td>0.682653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Support Vector Machine (Linear)</td>\n",
       "      <td>0.754977</td>\n",
       "      <td>0.730612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Support Vector Machine (Gaussian RBF)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.732653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.999745</td>\n",
       "      <td>0.771429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ensemble: Random Forest</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.825510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ensemble: Voting Classifier</td>\n",
       "      <td>0.956100</td>\n",
       "      <td>0.786735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Multi-Layer Perceptron</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Model  Train Accuracy  Test Accuracy\n",
       "0                                    KNN        0.757019       0.686735\n",
       "1                    Logistic Regression        0.757019       0.726531\n",
       "2         Polynomial Logistic Regression        0.785605       0.762245\n",
       "3                            Naive Bayes        0.712098       0.682653\n",
       "4        Support Vector Machine (Linear)        0.754977       0.730612\n",
       "5  Support Vector Machine (Gaussian RBF)        1.000000       0.732653\n",
       "6                          Decision Tree        0.999745       0.771429\n",
       "7                Ensemble: Random Forest        1.000000       0.825510\n",
       "8            Ensemble: Voting Classifier        0.956100       0.786735\n",
       "9                 Multi-Layer Perceptron        1.000000       0.785714"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\n",
    "    [\"KNN\", train_accuracy_knn, test_accuracy_knn],\n",
    "    [\"Logistic Regression\", train_accuracy_lg_reg, test_accuracy_lg_reg],\n",
    "    [\"Polynomial Logistic Regression\", train_accuracy_lg_reg_poly, test_accuracy_lg_reg_poly],\n",
    "    [\"Naive Bayes\", train_accuracy_nb, test_accuracy_nb],\n",
    "    [\"Support Vector Machine (Linear)\", train_accuracy_svm_linear, test_accuracy_svm_linear],\n",
    "    [\"Support Vector Machine (Gaussian RBF)\", train_accuracy_svm, test_accuracy_svm],\n",
    "    [\"Decision Tree\", train_accuracy_dtree, test_accuracy_dtree],\n",
    "    [\"Ensemble: Random Forest\", train_accuracy_rndforest, test_accuracy_rndforest],\n",
    "    [\"Ensemble: Voting Classifier\", train_accuracy_voting_clf, test_accuracy_voting_clf],\n",
    "    [\"Multi-Layer Perceptron\", train_accuracy_mlp, test_accuracy_mlp],\n",
    "    \n",
    "    ]\n",
    "pd.DataFrame(data, columns=[\"Model\", \"Train Accuracy\", \"Test Accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
